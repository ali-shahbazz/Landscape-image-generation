{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4669588,"sourceType":"datasetVersion","datasetId":2707450},{"sourceId":1217826,"sourceType":"datasetVersion","datasetId":298806}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torchvision tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:55:24.402643Z","iopub.execute_input":"2024-11-29T17:55:24.402998Z","iopub.status.idle":"2024-11-29T17:55:36.031084Z","shell.execute_reply.started":"2024-11-29T17:55:24.402967Z","shell.execute_reply":"2024-11-29T17:55:36.029705Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0+cpu)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: torch==2.4.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.4.0+cpu)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchvision) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchvision) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchvision) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchvision) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchvision) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0->torchvision) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0->torchvision) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\n# Paths to the datasets\ndataset_1_dir = \"/kaggle/input/landscape-recognition-image-dataset-12k-images/Landscape Classification/Landscape Classification/Training Data\"\ndataset_2_dir = \"/kaggle/input/landscape-pictures/\"\ncategories = [\"Coast\", \"Desert\", \"Forest\", \"Glacier\", \"Mountain\"]  # For Dataset 1\noutput_file = \"/kaggle/working/combined_landscape_captions.json\"\n\n# Initialize BLIP model and processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").cuda()\n\n# Function to generate caption\ndef generate_caption(image_path):\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(image, return_tensors=\"pt\").to(\"cuda\")\n        caption = model.generate(**inputs)\n        return processor.decode(caption[0], skip_special_tokens=True)\n    except Exception as e:\n        print(f\"Error processing {image_path}: {e}\")\n        return None\n\n# Process Dataset 1 by Categories\ndef process_dataset_1(image_caption_pairs):\n    for category in categories:\n        category_dir = os.path.join(dataset_1_dir, category)\n        if os.path.exists(category_dir):\n            print(f\"Processing category: {category}\")\n            for root, _, files in os.walk(category_dir):\n                for file in files:\n                    if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n                        image_path = os.path.join(root, file)\n                        caption = generate_caption(image_path)\n                        if caption:\n                            image_caption_pairs[image_path] = caption\n        else:\n            print(f\"Category directory {category_dir} not found.\")\n\n# Process Dataset 2\ndef process_dataset_2(image_caption_pairs):\n    print(\"Processing Dataset 2...\")\n    for root, _, files in os.walk(dataset_2_dir):\n        for file in files:\n            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n                image_path = os.path.join(root, file)\n                caption = generate_caption(image_path)\n                if caption:\n                    image_caption_pairs[image_path] = caption\n\n# Combine both datasets\ndef combine_datasets():\n    image_caption_pairs = {}\n    \n    # Process Dataset 1\n    process_dataset_1(image_caption_pairs)\n    \n    # Process Dataset 2\n    process_dataset_2(image_caption_pairs)\n    \n    # Save combined dataset to JSON\n    with open(output_file, \"w\") as f:\n        json.dump(image_caption_pairs, f, indent=4)\n    print(f\"Combined captions saved to {output_file}\")\n\n# Run the process\ncombine_datasets()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:17:40.247871Z","iopub.execute_input":"2024-11-29T18:17:40.248799Z","iopub.status.idle":"2024-11-29T19:02:26.583279Z","shell.execute_reply.started":"2024-11-29T18:17:40.248755Z","shell.execute_reply":"2024-11-29T19:02:26.582244Z"}},"outputs":[{"name":"stdout","text":"Processing category: Coast\nProcessing category: Desert\nProcessing category: Forest\nProcessing category: Glacier\nProcessing category: Mountain\nProcessing Dataset 2...\nCombined captions saved to /kaggle/working/combined_landscape_captions.json\n","output_type":"stream"}],"execution_count":5}]}